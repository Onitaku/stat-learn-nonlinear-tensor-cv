---
title: "Homework 1"
author: "PathFinders"
date: "08 aprile 2016"
output: html_document
---

```{r setup, include=FALSE, comment= F, warning=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, comment = NA,warning=FALSE)
library(rgl)
knit_hooks$set(webgl = hook_webgl)
```


# Part I: Linear VS Nonlinear (or greedy) approximations

## 1.  Legendre polynomials 

*Task: plot the first five (modified) Legendre polynomials. Verify, numerically, that they are orthonormal.*


Legendre basis are thought in order to create a countable base for *Hilbert* space, for functions defined in [-1, +1]:
 
 $$P_0(x) = 1, P_1(x) = x, P_2(x) = \frac{1}{2} (3x^2-1),\, . \, .\, ., Pj (x) = \frac{1}{2^j j!} \frac{d^j}{dx^j}(x^2-1)^j \, . \, .\, . $$
 
Since computing j-th derivate of $(x^2-1)^j$ would return a very long expression, it's usefull to express these polynomials explicitly through a recursive relation:

$$P_{j+1}(x) = \frac{(2j + 1) x P_j (x) - j P_{j-1}(x)}{j + 1}$$

```{r def legendre by hand}
legendre=function(x,j) {
  if(j==0) {
    return(1)
    }
  if(j==1) {
    return(x)
  }
  else return(
        (((2*j-1)*x*legendre(x,j-1))-(j-1)*legendre(x,j-2))/j)
}
```
These polynomials are orthogonal...

$$\int_{-1}^{+1}P_{j_1}(x)P_{j_2}(x)dx =0 \quad \forall \: j$$


```{r orthogonality}
a=integrate( function(x) legendre(x,5)*legendre(x,3), lower=-1,upper=1)$value
b=integrate( function(x) legendre(x,5)*legendre(x,3), lower=-1,upper=1)$value
e=integrate( function(x) legendre(x,5)*legendre(x,3), lower=-1,upper=1)$value
c(a,b,e)
```
...but not normal, since:

$$\int_{-1}^{+1}P^2_j(x)dx =\frac{2}{2j+1}$$

```{r integrate>1, echo=FALSE}
a=integrate(function(x) legendre(x,1)^2, lower = -1, upper = 1)$value
b=integrate(function(x) legendre(x,2)^2, lower = -1, upper = 1)$value
c=integrate(function(x) legendre(x,3)^2, lower = -1, upper = 1)$value
d=integrate(function(x) legendre(x,4)^2, lower = -1, upper = 1)$value
```
Actually:
 
$\int_{-1}^{+1}P^2_1(x)dx =$ `r a` \newline 
$\int_{-1}^{+1}P^2_2(x)dx =$ `r b` \newline 
$\int_{-1}^{+1}P^2_3(x)dx =$ `r c` \newline 
$\int_{-1}^{+1}P^2_4(x)dx =$ `r d` \newline

However, we can normalize Legendre polynomials:

$$Q_j(x) = \sqrt{\frac{(2j + 1)}{2}} P_j (x)$$

```{r normalized legendre}
q_x=function(x,j) {
  return(legendre(x,j)*sqrt((2*j+1)/2))
}
```

```{r plot q, echo=FALSE, fig.height=4}
q_x=Vectorize(q_x)

curve(q_x(x,0),xlim=c(-1,1),ylim=c(-2.5,2.5),lwd=3)
curve(q_x(x,1),col='blue',add=T,lwd=3)
curve(q_x(x,2),col='red',add=T,lwd=3)
curve(q_x(x,3),col='green',add=T,lwd=3)
curve(q_x(x,4),col='yellow',add=T,lwd=3)
curve(q_x(x,10),col='orange',add=T,lwd=3)
legend(x='top', legend = c('j=0','j=1','j=2','j=3','j=4','j=10'),lty = 1,lwd=3, ncol = 2, col=c('black','blue','red','green','yellow','orange'), bty = 'n')
```


... finally those will realize an **Orthonormal basis** for $L_2([-1, +1])$.
where $L_2$ is defined as: 
$$L_2([a, b]) = \{g : [a, b] \mapsto \mathbb{R} \:s.\: t. \left \| g \right \|_2 = \int_a^b\left | g(x)^2 \right | dx < \infty\}$$
```{r integrate=1, echo=FALSE}
a=integrate(function(x) q_x(x,1)^2, lower = -1, upper = 1)$value
b=integrate(function(x) q_x(x,2)^2, lower = -1, upper = 1)$value
c=integrate(function(x) q_x(x,3)^2, lower = -1, upper = 1)$value
d=integrate(function(x) q_x(x,4)^2, lower = -1, upper = 1)$value
```
indeed:

$\int_{-1}^{+1}P^2_1(x)dx =$ `r a` \newline 
$\int_{-1}^{+1}P^2_2(x)dx =$ `r b` \newline 
$\int_{-1}^{+1}P_1(x)P_4(x)dx =$  5.32524e-17\newline

We could say that averything seems ok... except for a tiny little *silly* thing: It's not going to work!
Indeed the recursive function we have defined before, has two *parallel loops* ( e.g. for the j-th base it needs to go through, *almost*, $j^2$ levels ) so for the sake of our PC we should parellelize calculations assigning them to different threads/core, or caching the function ... or just use the *gsl* library...


```{r lib Legendre}
library(gsl)
leg.basis2 <- function(j,x) sqrt((2*j + 1)/2)*legendre_Pl(j, x)
```

...Basis are ready, now let's set up the target function: *Doppler function* rescaled to [-1,1]:

$$g(x) =\sqrt{(x(1-x))}sin(\frac{2.1\pi}{x+0.05})$$
$$h(x)= g(0.5(x+1))$$

```{r Dopp, echo=FALSE, fig.height=4}
doppler.fun <- function(x){
  out = sqrt(x*(1 - x))*sin( (2.1*pi)/(x + 0.05) )
  return(out)
}

h_x=function(x) {
  return(doppler.fun(0.5*(x+1)))
}

curve(doppler.fun(x),xlim=c(-1,1),ylim=c(-1,1))
curve(h_x(x),add = T, col='red')

```


#### Fourier Coefficient

Evaluate the Fourier coefficients of the Doppler under our Cosine-basis:

$$\beta_{j}=\left \langle g,\phi_j  \right \rangle_{L_2}= \int_{-1}^{+1}g(x)\phi_j(x)dx$$


```{r coeff, echo = F, fig.height=3.5}

j.max <- 200
f.coeff <- rep(NA, j.max)

par(mfrow=c(1,1))
curve(leg.basis2(0,x), col=rainbow(41)[1], xlim = c(-1,1), ylim=c(-2,2),
      main='Normalized Legendre Basis (0:40)')

for (i in 1:20) {
  curve(leg.basis2(i,x), col=rainbow(21)[i+1], add=T)
}

for (idx in 1:j.max){
  foo = tryCatch(
    integrate(function(x) h_x(x) * leg.basis2((idx-1), x),
              lower = -1, upper = 1 )$value,
    error = function(e) NA
  )
  f.coeff[idx] = foo
}


plot(f.coeff, type = "h",
     main = "Doppler function, Legendre Basis:  Fourier coefficients",
     sub = "Legendre-basis", ylab = expression(tilde(f)(x)), ylim=c(-0.15,0.15)
     )
```

First 10 coefficients:
```{r echo=F}
head(f.coeff,10)
```

\newpage 

#### Linear approximation

Approximation for different n-terms approximations J={5, 10, 25, 50, 100, 150} as follow:

$$g_{J}(x)=\sum_{j=0}^{J-1}\beta_j\phi_j(x)$$

and evaluating the error:

$$Error= \left \| h(x)-h_{J}(x) \right \|^2_{L_2}= \int_{-1}^{+1}(h(x)-h_{J}(x))^2dx$$

```{r predictions, echo = F}
proj.legendre= function(x, f.coeff, j.max){
  out = rep(0, length(x))
  for(idx in 0:(j.max-1)){
    if ( !is.na(f.coeff[idx + 1]) ){
      out = out + f.coeff[idx + 1] * leg.basis2(idx, x)
    }
  }
  return(out)
  
}  
# Visualize some n-terms approximations
j.seq = c(5, 10, 25, 50, 100, 200)
library(viridis)
mycol = viridis(length(j.seq), alpha = .7)

error= function(x, f.coeff, j.max){
  h_x(x)-proj.legendre(x, f.coeff, j.max)
}

# Divide the graphical device
par(mfrow = c(2,3))
for (idx in 1:length(j.seq)){
  e=integrate(function(x) error(x,f.coeff,j.seq[idx])^2, -1,1)$value
  # Original function
  curve(h_x(x), from = -1, to = 1,
        main = paste(j.seq[idx], "-term approximation", sep = ""),
        sub = bquote("Error"  == .(round(e, 7))),
        ylab = expression(tilde(f)(x)),
        ylim=c(-1,1),
        n = 1001, col = 'red', lwd = 2)
  
  # Add approximation
  curve(proj.legendre(x, f.coeff, j.seq[idx]),
        n = 1001, col = mycol[idx], lwd = 3,
        add = TRUE)
}

```

\newpage 

#### Non-Linear approximation

A little improvement can be seen if we select the largest coefficient, in order to use only the more relevant function!

$$g_{J}^*(x)=\sum_{j\in \Lambda_{J}}\beta_j\phi_j(x)$$

```{r nonlinear, echo=F}
par(mfrow = c(2,3))

for (idx in 1:length(j.seq)){
  f.coeff.top=f.coeff
  f.coeff.top[abs(f.coeff.top)<(sort(abs(f.coeff), decreasing = T)[j.seq[idx]])]=NA
  e=integrate(function(x) error(x,f.coeff.top,200)^2, -1,1)$value
  
  # Original function
  curve(h_x(x), from = -1, to = 1,
        main = paste('top ',j.seq[idx], " largest coefficient", sep = ""),
        sub = bquote("Error"  == .(round(e, 7))),
        ylab = expression(tilde(f)(x)),
        ylim=c(-1,1),
        n = 1001, col = 'red', lwd = 2)
  
  # Add approximation
  curve(proj.legendre(x, f.coeff.top, 200),
        n = 1001, col = mycol[idx], lwd = 3,
        add = TRUE)
}


```

Just to check that everything is working, notice that linear & non linear on the first 200 terms recover the same error value!


\newpage
# Part II: Tensor Product Models & 3D Plots

## 1.  Building the Tensor Product Basis 

*Task: Define all the functions needed to build the approximant function, learning, along the way, how to numerically evaluate double integrals.*


We can build the Tensor basis using the definition provided:
 
 $$\phi_{j1,j2}(x_1,x_2)=\phi_{j1}(x_1)\phi_{j2}(x_2) \enspace j1,j2=0,1...$$
 
for the cosine basis defined in previous labs

$$\phi_{0,0}=1  \enspace     \phi_{j1,j2}=\sqrt{2}cos(j_1\pi x)\sqrt{2}cos(j_2 \pi x)$$
 
```{r tensor basis}
tensor.cosbasis = function(x, j1,j2){
  (1*(j1 == 0) + sqrt(2)*cos(pi*j1*x[1])*(j1 > 0))*
    (1*(j2 == 0) + sqrt(2)*cos(pi*j2*x[2])*(j2 > 0))
}
```

We managed to evaluate the performance of double integrals libraries in order to speed up the computation of forthcoming steps. We tried two libraries: cubature and R2Cuba. The second library performed better. Thanks to the double integral library we can check the normality and orthogonality of our basis:

```{r check norm and orth}

library(cubature)
library(R2Cuba)
## Check normality
adaptIntegrate(function(x) tensor.cosbasis(x,1,2)^2, lowerLimit = c(0,0), 
               upperLimit = c(1,1),maxEval = 50000)
cuhre(2,1,function(x) tensor.cosbasis(x,1,2)^2,lower = c(0,0),upper = c(1,1))

## Check othogonality
prod.base.base <- function(x,j1,j2) {
  tensor.cosbasis(x,j1[1],j1[2])*tensor.cosbasis(x,j2[1],j2[2])
}

adaptIntegrate(function(x) prod.base.base(x,c(1,2),c(1,2)), lowerLimit = c(0,0), 
               upperLimit = c(1,1),maxEval = 50000)
cuhre(2,1,function(x) prod.base.base(x,c(1,2),c(1,2)),lower = c(0,0),upper = c(1,1))
```

Now that we built our basis we can compute the Fourier Coefficients defined as:


$$\beta_{j1,j2}=\left \langle g(x_1,x_2),\phi_{j1,j2}(x_1,x_2)  \right \rangle_{L_2}= \int_{0}^{1}\int_{0}^{1}g(x_1,x_2),\phi_{j1,j2}(x_1,x_2)dx_1dx_2$$


```{r coefficients, eval=FALSE}
g_x=function(x) {
  
  x[1]+cos(x[2])
}


j1.max <- 50
j2.max <- 50
f.coeff <- matrix(NA, j1.max, j2.max)


for (idx1 in 0:(j1.max-1)){
  for (idx2 in 0:(j2.max-1)){
    coeff = tryCatch(
      cuhre(2,1,function(x) prod.base.g(x,idx1,idx2),lower = c(0,0),upper = c(1,1),
            flags = list(verbose=0))$value,
                     error = function(e) NA
                    )
          f.coeff[idx1+1,idx2+1] = coeff
  }
}
```

It's a long computation for the PC, so we have computed once and stored them into a file: 'fcoeff.txt'. So you need just to read.table('fcoeff.txt') them. Furthermore thanks to our i7 Mac processor, we didn't need to set a max number of evaluation for the cuhre function :)



```{r plot predi, echo=FALSE, webgl=TRUE}

f.coeff=read.table('fcoeff.txt')

g_x=function(x) {
  
  x[1]+cos(x[2])
}
predict = function(x,J1,J2){
  out = 0
  for(j1 in 1:J1){
    for(j2 in 1:J2){
      if ( !is.na(f.coeff[j1,j2])){
        out = out + f.coeff[j1,j2] * tensor.cosbasis(x,j1-1,j2-1) 
      }
    }
  }
  return(out)
}

```



Now in order to proceed, instead of evalueate the function in just three combinations of J1 and J2, we decided to observe the risk function behaviour in 4 blocks cases, first two with just any coefficents, second two with more coefficients, and then two particulat case: just 2 and all of them . Per each block we set the total amount of coefficients to use, and then try different combination ( j1<j2, j1=j2, j1>j2)... in this way we can observe that.... the functions it's so smooth that no matter what couple you choose ( homogeneous or  not ) the error is going down proportionally to the approximation order.
Although the min error is around 37*37 coefficients... thus don't go too far


```{r errori differenti j, echo= F}

err=data.frame(read.table('error.txt'))

colnames(err)=c('j1','j2','err')
err
```


#### (5,5)
```{r sono 5 5 , webgl=TRUE}
library(plot3D)
library(rgl)
library(rglwidget)

#Define sequences of X and Y in order to evaluate the function
X <- seq(0, 1, length = 100)
Y <- X
count=length(X)
# Z is a matrix of the results of the function evaluated in X and Y 
Z=matrix(nrow=count,ncol=count)
for (i in 1:count) {
  for (k in 1:count) {
    Z[i,k]=g_x(c(X[i],Y[k]))
  }
}
Z[is.na(Z)] <- 1

# We open the 3d plot environment
open3d()
bg3d("white")
material3d(col = "black")
# Plot the function
persp3d(X,Y,Z,  col = "lightgrey",
        xlab = "X", ylab = "Y", zlab = "g(x)")




X1 <- seq(0, 1, length = 100)
Y1 <- X1
count2=length(X1)
Z1=matrix(nrow=count2,ncol=count2)
for (i in 1:count2) {
  for (k in 1:count2) {
           Z1[i,k]=predict(c(X1[i],Y1[k]),5,5)
           }
      }

bg3d("white")
nbcol = 100
color = rev(rainbow(nbcol, start = 0/6, end = 4/6))
zcol  = cut(Z1, nbcol)
persp3d(X1, Y1, Z1, col = color[zcol],
        xlab = "X", ylab = "Y", zlab = "prediction",add=T)


```


#### (7,3)
```{r 7 3, webgl=TRUE}
library(plot3D)
library(rgl)
library(rglwidget)

#Define sequences of X and Y in order to evaluate the function
X <- seq(0, 1, length = 100)
Y <- X
count=length(X)
# Z is a matrix of the results of the function evaluated in X and Y 
Z=matrix(nrow=count,ncol=count)
for (i in 1:count) {
  for (k in 1:count) {
    Z[i,k]=g_x(c(X[i],Y[k]))
  }
}
Z[is.na(Z)] <- 1

# We open the 3d plot environment
open3d()
bg3d("white")
material3d(col = "black")
# Plot the function
persp3d(X,Y,Z,  col = "lightgrey",
        xlab = "X", ylab = "Y", zlab = "g(x)")




X1 <- seq(0, 1, length = 100)
Y1 <- X1
count2=length(X1)
Z1=matrix(nrow=count2,ncol=count2)
for (i in 1:count2) {
  for (k in 1:count2) {
           Z1[i,k]=predict(c(X1[i],Y1[k]),7,3)
           }
      }

bg3d("white")
nbcol = 100
color = rev(rainbow(nbcol, start = 0/6, end = 4/6))
zcol  = cut(Z1, nbcol)
persp3d(X1, Y1, Z1, col = color[zcol],
        xlab = "X", ylab = "Y", zlab = "prediction",add=T)


```



#### (37,37)
```{r 37 37, webgl=TRUE}
library(plot3D)
library(rgl)
library(rglwidget)

#Define sequences of X and Y in order to evaluate the function
X <- seq(0, 1, length = 100)
Y <- X
count=length(X)
# Z is a matrix of the results of the function evaluated in X and Y 
Z=matrix(nrow=count,ncol=count)
for (i in 1:count) {
  for (k in 1:count) {
    Z[i,k]=g_x(c(X[i],Y[k]))
  }
}
Z[is.na(Z)] <- 1

# We open the 3d plot environment
open3d()
bg3d("white")
material3d(col = "black")
# Plot the function
persp3d(X,Y,Z,  col = "lightgrey",
        xlab = "X", ylab = "Y", zlab = "g(x)")




X1 <- seq(0, 1, length = 100)
Y1 <- X1
count2=length(X1)
Z1=matrix(nrow=count2,ncol=count2)
for (i in 1:count2) {
  for (k in 1:count2) {
           Z1[i,k]=predict(c(X1[i],Y1[k]),37,37)
           }
      }

bg3d("white")
nbcol = 100
color = rev(rainbow(nbcol, start = 0/6, end = 4/6))
zcol  = cut(Z1, nbcol)
persp3d(X1, Y1, Z1, col = color[zcol],
        xlab = "X", ylab = "Y", zlab = "prediction",add=T)


```





\newpage
# Part III: Variable Selection & Cross-Validation

## Correlation between Data

```{r, echo =F}
#1) Loading the data
dat<-read.table("PET.txt",header=T)
library(viridis)

attach(dat)

#Splitting the train and the test set
pet.train<-dat[train==1,-270]
pet.test<-dat[train==0,-270]

par(mfrow = c(2,3))
matplot( dat[,1:25], 
         type = "l", lty = 1, lwd = 2.5,
         col = rainbow(25),
         main = "Near-Infrared Spectra 1:25",
         ylab = "Raman near-infrared",
          yaxt = "n"
)
matplot( dat[,25:50], 
         type = "l", lty = 1, lwd = 2.5,
         col = rainbow(25),
         main = "Near-Infrared Spectra 25:50",
         ylab = "Raman near-infrared",
          yaxt = "n"
)
matplot( dat[,50:75], 
         type = "l", lty = 1, lwd = 2.5,
         col = rainbow(25),
         main = "Near-Infrared Spectra 50:75",
         ylab = "Raman near-infrared",
          yaxt = "n"
)
matplot( dat[,75:150], 
         type = "l", lty = 1, lwd = 2.5,
         col = rainbow(75),
         main = "Near-Infrared Spectra 75:150",
         ylab = "Raman near-infrared",
          yaxt = "n"
)
matplot( dat[,150:200], 
         type = "l", lty = 1, lwd = 2.5,
         col = rainbow(50),
         main = "Near-Infrared Spectra 150:200",
         ylab = "Raman near-infrared",
         yaxt = "n"
)
matplot( dat[,200:268], 
         type = "l", lty = 1, lwd = 2.5,
         col = rainbow(68),
         main = "Near-Infrared Spectra 200:268",
         ylab = "Raman near-infrared",
         yaxt = "n"
)
```
```{r echo=F, fig.height=2}
plot(rep(1,268),xlab = "Frequency",ylab = '',yaxt = "n",xaxt='n',xlim=c(0,260),ylim=c(0.9,1.1),col=rainbow(268), pch=20,cex=2, lwd=10)

```


Lets take a look to the correlation between the covariates
```{r}
X.cor <- cor(dat[,1:268])
mean(abs(X.cor))


```

...very high (average) correlation, in particular between values observed at nearby frequencies (...quite naturally...). We gonna have some problems here...


```{r , echo=FALSE}

image( X.cor, col = inferno(72), axes = F,
       main = "Covariates correlation matrix")
remove(predict)

matplot( t(dat[,1:268]), 
         type = "l", lty = 1, lwd = 2.5,
         col = rainbow(28),
         main = "Near-Infrared Spectra",
         xlab = "Frequencies",
         ylab = "density of the yarn",
         yaxt = "n"
)

```


So in conclusion, first 6 graph showed us that values os NIR are related one to each other for close freq... but also (last one) the information related to some frequency is redoundant, because the curves are often parallel.. so we should drop a lot of them

```{r}


#Fitting the model on the train test
m<-lm(y~.,data=pet.train)
#Summaries of the model
head(m$coefficients,28)
```

We are encountering the *small (n) - large (p)* problem where the parameters,i.e., 
the coefficients of the independent
variables are way larger than the sample size. 
This affects the degrees of freedom of the residuals and thus 
we are getting NA for the most estimates and for the statistics
(F statistic since we are getting negative degrees of freedom)

### Step forward selection

Since there's a different in the scale of values between the variates and the covariates we decided to scale the $\mathbb{X}$ 
```{r}
######
ytr<-pet.train[ , 269] 
Xtr = pet.train[ , -c(269,270)] 
yte = pet.test[ , 269] 
Xte = pet.test[ , -c(269,270)]
#Standardising the independent variables
Ztr = scale(Xtr)
Zte = scale(Xte)

```

* *Definde FWD*:

```{r}
fwd.reg <- function(y, X, k.max = ncol(X), stand = TRUE){
  # Standardize if asked 
  if (stand) X <- scale(X) 
  # Initialize variable sets & other quantities
  S = NULL 
  # active set
  U = 1:ncol(X) 
  #inactive set 
  k.loc = 0 
  # current active set dim
  ee = y 
  #Loop
  while (k.loc < k.max){
    ## Update loop-index 
    k.loc = k.loc + 1 
    ## Step 1: Evaluate correlation with inactive variables 
    rr = abs( cor(X[,U], ee) )
    ## Step 2: Extract the most correlated variable & update the sets
    J = U[which.max(rr)]
    S = c(S, J) 
    U = U[-which.max(rr)] 
    ## Step 3: Regress on active var's and get residuals
    ee = resid( lm(y ~ X[,S]) ) } 
  # Output
  return(S) 
}

S<-fwd.reg(ytr,Xtr) 
S

```
Now we have an order of the most correlated independent variables with the dependent. 
The rest were not chosen because of independence, indeed we have selected the covariates that in each step have some relation with the residuals of the model, so in each step we add relevant information that can lower the error!!!
Infact having a similar *trend* with the residuals will plug into the madel  the variable concerned.
Although it depends on the previous choice, infact we cannot claim that this is the best model, but a *local* best model, maybe another set of variables could do better but how could we find it?
Infact in each step we go to the local minimization of the error, looking at the relation with the residuals..


```{r}
# Initialize the vectors of scores 
model.score <- matrix(NA, length(S),4)

#y hat
m=lm(ytr ~ Ztr)
yhat <- predict(m, data.frame(Ztr) ) 
train.error=ytr-yhat

library(MASS)
for (idx in 1:length(S)){
  
  
  # Build the data.frame with the FWD-selected variables
  ztr <- Ztr[, S[1:idx], drop = FALSE] 
  zte <- Zte[, S[1:idx], drop = FALSE] 
  xtr <- Xtr[, S[1:idx], drop = FALSE]
  xtr=data.matrix(xtr)
  
  ####Train-Split Schema
  
  m=lm(ytr ~ ztr)
  yhat <- predict(m, newdata=list(ztr=zte ) )
  MSE=mean((yte - yhat)^2)
  
  #### Leave One Out
  yhat <- predict(m, newdata=list(ztr=ztr ) )
  hat=function(x) diag(x%*%ginv(t(x)%*%x)%*%t(x))
  
  R.LOO=1/length(ytr)*sum(((ytr - yhat)/(1-hat(ztr)))^2)
  
  
  #### Generalized  Cross-Validation
  
  R.GCV= 1/20*sum((ytr - yhat)^2)/((1-(idx/length(train.error)))^2)
  
  #Cross Validation
  nfold<-5
  rarr<-sample(1:nrow(ztr))
  rarr
  fold.str<-matrix(rarr, nrow = nfold, ncol = round(nrow(ztr)/nfold))
  fold.str
  ########      NB sono 21, fold da 5 ---> ne scarti uno (la 20 riga nel nostro caso)
  score = c()
  for (i in 1:nfold) {
    test.idx<-fold.str[i,]
    test.loc<-ztr[test.idx,]
    test.y=ytr[test.idx]
    train.idx = c(fold.str[-i,])
    train.loc = ztr[train.idx,]
    train.y=ytr[train.idx]
    m.loc = lm(train.y ~ train.loc)
    y.hat = predict(m.loc, newdata=list(train.loc=test.loc))
    score[i] = mean(y.hat - test.y)^2
  }
  cv.score = mean(score)
  model.score[idx,] = c(MSE,cv.score,R.LOO,R.GCV)
}

```

```{r echo=F}

# Optimal number of covariates
n.opt=data.frame(row.names = c('Train-test split Scheme','5-fold CV','Leave-1-out','Generalized CV') )
final.rmse=data.frame(row.names = c('k=3 Tr-Te','k=12 5-CV','k=20 LOO','k=20 GCV'))
for (i in 1:4){
  k.opt = which.min(model.score[,i])
  n.opt[i,1]=k.opt
  ztr=Ztr[,S[1:k.opt], drop=F]
  zte=Zte[,S[1:k.opt], drop=F]
  m=lm(ytr ~ ztr)
  final.rmse[i,1]=sqrt(mean((yte - predict(m, newdata = list(ztr=zte)))^2))
}
names(n.opt)='Optimal number of cov.'
names(final.rmse)='RMSE'
titles=c('Train-test split Scheme','5-fold CV','Leave-1-out','Generalized CV')
for (i in 1:4){
plot(model.score[,i], type = 'l',pch = 21, bg = "yellow", cex = .7, main =titles[i], ylab = "RSS-Test") 
points(n.opt[i,1], model.score[n.opt[i,1],i], pch = 21, cex = 1.5, 
       col = "darkred", bg = rgb(1,0,0,.3),
       text(n.opt[i,1]-(i-1), model.score[n.opt[i,1],i]-(i-3.5)*3,
      paste(expression(k[opt]),'=',n.opt[i,1]), cex = .7, pos = 3)) 
grid()
}
n.opt
final.rmse
```

Generally we used the best 20 correlated variables. That was in order to conduct the regression by keeping the degrees of freedom positive. The optimal number of variables that is choosen depends on wich criteria we use to evaluate the model... In the end the best model achieved with this algorithm is the one with X.40, X.241 and X.1.
We cannot claim this is the best model... Infact there have been a lot of incongruences in our procedure, for example extimate parameters and compute the error on the same data.






